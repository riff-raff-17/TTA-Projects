{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db51d9cb",
   "metadata": {},
   "source": [
    "### MountainCar-v0 with Gymnasium + Stable-Baselines3 (Step-by-Step)\n",
    "\n",
    "In this notebook you'll:\n",
    "\n",
    "- Inspect the MountainCar-v0 environment (state, actions, rewards)\n",
    "\n",
    "- Run a random policy to build intuition\n",
    "\n",
    "- Train a DQN agent with Stable-Baselines3\n",
    "\n",
    "- Evaluate, record a short video, and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb21db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "print(\"Stable-Baselines3:\", sb3.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cd723",
   "metadata": {},
   "source": [
    "### Meet MountainCar-v0\n",
    "\n",
    "State (observation): [position, velocity] (shape: 2)\n",
    "\n",
    "Actions: {0: push left, 1: no push, 2: push right}\n",
    "\n",
    "Goal: Drive the underpowered car up the right hill (position ≥ 0.5).\n",
    "\n",
    "Reward: −1 per step until the goal is reached (shorter episodes = better).\n",
    "\n",
    "Episode ends: goal reached or after 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a029e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"Initial observation:\", obs, \" | info:\", info)\n",
    "print(\"Sample action:\", env.action_space.sample())\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02486180",
   "metadata": {},
   "source": [
    "**Roll out one random episode**\n",
    "\n",
    "This helps you see how the state evolves and why naive actions fail.\n",
    "We'll track the car's position over time and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "positions = []\n",
    "rewards = []\n",
    "obs, info = env.reset(seed=123)\n",
    "\n",
    "done = False\n",
    "total_r = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    positions.append(obs[0])\n",
    "    rewards.append(reward)\n",
    "    total_r += reward\n",
    "\n",
    "env.close()\n",
    "print(\"Episode length:\", len(rewards), \" | Total reward:\", total_r)\n",
    "\n",
    "# Plot position over steps\n",
    "plt.figure()\n",
    "plt.plot(positions)\n",
    "plt.title(\"Random Policy: Car Position over Time\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80efb",
   "metadata": {},
   "source": [
    "Add logging and record a short video\n",
    "\n",
    "Monitor saves per-episode rewards/lengths.\n",
    "\n",
    "RecordVideo (from Gymnasium) saves .mp4 rollouts for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "log_dir = \"./mountaincar_logs\"\n",
    "video_dir = \"./mountaincar_videos\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Monitored env for training stats\n",
    "def make_env(seed=0):\n",
    "    e = gym.make(\"MountainCar-v0\", render_mode=None)\n",
    "    e = Monitor(e, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "    return e\n",
    "\n",
    "# A separate env only for video (avoid slowing down training)\n",
    "def make_video_env(seed=123):\n",
    "    e = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "    e = RecordVideo(e, video_folder=video_dir, episode_trigger=lambda ep: True)\n",
    "    return e\n",
    "\n",
    "print(\"Log dir:\", log_dir)\n",
    "print(\"Video dir:\", video_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb8da7",
   "metadata": {},
   "source": [
    "Sample video! The car will move randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create a video-enabled environment\n",
    "video_test_dir = \"./sample_video\"\n",
    "os.makedirs(video_test_dir, exist_ok=True)\n",
    "\n",
    "env_video = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "env_video = RecordVideo(env_video, video_folder=video_test_dir, episode_trigger=lambda ep: True)\n",
    "\n",
    "obs, info = env_video.reset(seed=42)\n",
    "done = False\n",
    "while not done:\n",
    "    action = env_video.action_space.sample()  # random action\n",
    "    obs, reward, terminated, truncated, info = env_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env_video.close()\n",
    "print(\"Sample video saved to:\", os.path.abspath(video_test_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09acf61",
   "metadata": {},
   "source": [
    "A tiny callback to save the best model\n",
    "\n",
    "We'll compute a simple moving mean of rewards using Monitor logs and save the best-performing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        if (self.n_calls % self.check_freq) == 0:\n",
    "            try:\n",
    "                # Load results and compute mean reward\n",
    "                x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "                if len(y) > 0:\n",
    "                    mean_reward = np.mean(y[-100:])\n",
    "                    if self.verbose:\n",
    "                        print(f\"Step {self.n_calls} | Last-100 mean reward: {mean_reward:.2f}\")\n",
    "                    if mean_reward > self.best_mean_reward:\n",
    "                        self.best_mean_reward = mean_reward\n",
    "                        path = os.path.join(self.save_path, \"best_model\")\n",
    "                        self.model.save(path)\n",
    "                        if self.verbose:\n",
    "                            print(f\"✔ Saved new best model to {path}\")\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(\"Callback warning:\", e)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7597117",
   "metadata": {},
   "source": [
    "### Train a DQN agent\n",
    "\n",
    "A good, simple baseline for discrete-action tasks like MountainCar. We'll train for a modest number of timesteps so it runs quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = make_env()\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env_train,\n",
    "    learning_rate=4e-3,\n",
    "    buffer_size=10_000,\n",
    "    learning_starts=1_000,\n",
    "    batch_size=128,\n",
    "    gamma=0.98,\n",
    "    train_freq=16,\n",
    "    gradient_steps = 8,\n",
    "    target_update_interval=600,\n",
    "    exploration_fraction=0.2,   # explore for ~20% of training\n",
    "    exploration_final_eps=0.07,\n",
    "    verbose=1,\n",
    "    tensorboard_log=os.path.join(log_dir, \"tb\")\n",
    ")\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=5_000, log_dir=log_dir, verbose=1)\n",
    "timesteps = 300_000  # try 200_000+ for better results\n",
    "model.learn(total_timesteps=timesteps, callback=callback)\n",
    "model.save(os.path.join(log_dir, \"final_model\"))\n",
    "env_train.close()\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aeb1c8",
   "metadata": {},
   "source": [
    "### Evaluate the policy\n",
    "\n",
    "We measure average return over several episodes. MountainCar returns are negative; less negative is better. Success corresponds to quickly reaching the goal (position ≥ 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"MountainCar-v0\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, render=False, deterministic=True)\n",
    "eval_env.close()\n",
    "print(f\"Mean reward over 10 episodes: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377c327",
   "metadata": {},
   "source": [
    "### Watch a rollout (video)\n",
    "\n",
    "We'll load the best checkpoint if available, otherwise the final model, and record one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load best checkpoint\n",
    "best_path = os.path.join(log_dir, \"best_model\", \"best_model.zip\")\n",
    "if os.path.exists(best_path):\n",
    "    print(\"Loading best checkpoint:\", best_path)\n",
    "    model = DQN.load(best_path)\n",
    "else:\n",
    "    print(\"Best checkpoint not found; using final model.\")\n",
    "    model = DQN.load(os.path.join(log_dir, \"final_model.zip\"))\n",
    "\n",
    "vid_env = make_video_env()\n",
    "obs, info = vid_env.reset(seed=2025)\n",
    "done = False\n",
    "ep_reward = 0\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = vid_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    ep_reward += reward\n",
    "vid_env.close()\n",
    "print(\"Recorded one episode. Reward:\", ep_reward)\n",
    "print(\"Check the video folder:\", os.path.abspath(\"mountaincar_videos\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ed5cb",
   "metadata": {},
   "source": [
    "### Plot the learning curve\n",
    "\n",
    "We'll parse the Monitor log and plot episode rewards over training timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadeddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "monitor_files = glob(os.path.join(log_dir, \"*.monitor.csv\")) + glob(os.path.join(log_dir, \"monitor.csv\"))\n",
    "if not monitor_files:\n",
    "    print(\"No monitor file found. Did training run?\")\n",
    "else:\n",
    "    # Load Monitor CSV, skip header & comments\n",
    "    df = pd.read_csv(\n",
    "        monitor_files[0],\n",
    "        skiprows=2,\n",
    "        comment=\"#\",\n",
    "        names=[\"r\", \"l\", \"t\"]  # r: reward, l: length, t: time\n",
    "    )\n",
    "    df = df.astype(float)\n",
    "    df[\"timesteps\"] = df[\"l\"].cumsum()\n",
    "\n",
    "    # Moving average over rewards\n",
    "    window = 50\n",
    "    df[\"r_smooth\"] = df[\"r\"].rolling(window, min_periods=1).mean()\n",
    "\n",
    "    display(df.head())\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df[\"timesteps\"], df[\"r\"], alpha=0.3, label=\"Raw rewards\")\n",
    "    plt.plot(df[\"timesteps\"], df[\"r_smooth\"], label=f\"Smoothed (window={window})\")\n",
    "    plt.title(\"Episode Reward vs Timesteps\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Episode Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
