{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d6f92a",
   "metadata": {},
   "source": [
    "# Q-Learning on Taxi-v3 (Gymnasium)\n",
    "\n",
    "In this notebook, we train a **Q-learning** agent to solve the classic **Taxi-v3** environment from Gymnasium.  \n",
    "The task takes place on a 5×5 grid, where a taxi must **pick up** a passenger at one location and **drop off** the passenger at a designated destination.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "Learn an optimal **policy** that maximizes expected rewards under the Taxi-v3 rules:\n",
    "- **−1** per step (to encourage efficiency)  \n",
    "- **−10** for illegal pickup or drop-off actions  \n",
    "- **+20** for successfully dropping off the passenger at the goal\n",
    "\n",
    "---\n",
    "\n",
    "### What You’ll Do\n",
    "1. Initialize a **Q-table** for all state–action pairs (500 states × 6 actions).  \n",
    "2. Train the agent using **ε-greedy exploration** and the **Q-learning update rule**.  \n",
    "3. Track and visualize **learning progress** over episodes.  \n",
    "4. Evaluate the **greedy policy** (no exploration) to measure performance.  \n",
    "5. Decode and inspect the learned **state–action behavior** to understand the strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2165a6",
   "metadata": {},
   "source": [
    "## Environment at a Glance\n",
    "\n",
    "**Taxi-v3** is a discrete MDP on a 5×5 grid. The taxi must **pick up** a passenger at one of four landmark locations and **drop off** at a specified destination.\n",
    "\n",
    "- **Observation space:** 500 discrete states  \n",
    "  - State encodes: **taxi row (5)** × **taxi column (5)** × **passenger location (5: R/G/B/Y or in taxi)** × **destination (4: R/G/B/Y)** = 500\n",
    "- **Action space (6):**\n",
    "\n",
    "  0. South  \n",
    "  1. North  \n",
    "  2. East  \n",
    "  3. West  \n",
    "  4. Pickup  \n",
    "  5. Dropoff\n",
    "- **Rewards:**\n",
    "  - **+20** for a successful dropoff at the correct destination  \n",
    "  - **−1** per time step (encourages efficiency)  \n",
    "  - **−10** for an illegal **Pickup/Dropoff** (e.g., wrong location or no passenger)\n",
    "- **Episode termination:** when the passenger is dropped at the destination (success) or when the episode times out (env’s max steps).\n",
    "- **Dynamics:** deterministic moves on the grid (walls block motion between some cells).\n",
    "\n",
    "> Tip: You can `env.render()` (text mode) during evaluation to visualize the taxi (`T`), passenger source, and destination cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional: quick version check\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Environment & Inspect Spaces\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "state, info = env.reset(seed=SEED)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"Initial state: {state}\")\n",
    "print(f\"Number of states: {n_states}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4681cd",
   "metadata": {},
   "source": [
    "## Q-Learning Refresher\n",
    "\n",
    "**Q-learning** is an off-policy, model-free reinforcement learning algorithm used to learn the value of taking a specific action in a given state.\n",
    "\n",
    "At each step, the agent updates its estimate of the Q-value using the **Bellman equation**:\n",
    "\n",
    "\n",
    "The agent balances **exploration** and **exploitation** via an **ε-greedy policy**:\n",
    "- With probability **ε**, choose a random action (explore).  \n",
    "- With probability **1 − ε**, choose the action with the highest Q-value (exploit).\n",
    "\n",
    "Over time, as ε decays, the policy becomes greedier, converging toward optimal behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table & Hyperparameters\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.99 # Discount factor\n",
    "epsilon = 1.0 # Exploration rate (start fully random)\n",
    "epsilon_min = 0.05 # Minimum exploration rate\n",
    "epsilon_decay = 0.995\n",
    "episodes = 5000 # Change episodes as needed\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "print(\"Q-table shape:\", Q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Utilities\n",
    "def moving_average(x, window=100):\n",
    "    \"\"\"Compute moving average for smoothing rewards.\"\"\"\n",
    "    if len(x) < window:\n",
    "        return np.array(x, dtype=float)\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[window:] - cumsum[:-window]) / float(window)\n",
    "\n",
    "def epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Select an action using the epsilon-greedy strategy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(n_actions) # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state]) # Exploit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * Q[next_state, best_next_action] - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(f\"Episode {episode + 1}/{episodes}, epsilon={epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8384534",
   "metadata": {},
   "source": [
    "## Training Notes & Pitfalls\n",
    "\n",
    "A few things to keep in mind while training the Taxi-v3 Q-learning agent:\n",
    "\n",
    "- **Sparse Rewards:** The agent only receives a large positive reward (+20) at the end of a successful episode, so early in training it may seem like it’s not learning much.  \n",
    "- **Exploration Decay:** If ε decays too quickly, the agent may stop exploring before finding good trajectories. If it decays too slowly, convergence will be slower.  \n",
    "- **Learning Rate (α):** Too high, and learning becomes unstable; too low, and convergence is painfully slow.  \n",
    "- **Discount Factor (γ):** Controls how much the agent values future rewards. A value close to 1 encourages longer-term planning.  \n",
    "- **Illegal Actions:** Picking up or dropping off at the wrong place gives −10. The agent must learn to avoid these.  \n",
    "- **Episode Length:** Taxi-v3 can require many steps per episode before success. Make sure `max_steps_per_episode` is sufficient for meaningful exploration.\n",
    "\n",
    "> Early learning curves will fluctuate heavily. As Q-values stabilize, the moving average of rewards should gradually trend upward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves\n",
    "# Compute moving average of rewards for smoothing\n",
    "window_size = 100\n",
    "smoothed_rewards = moving_average(rewards_per_episode, window_size)\n",
    "\n",
    "# Plot learning progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label=\"Episode Reward\", alpha=0.4)\n",
    "plt.plot(range(window_size - 1, len(smoothed_rewards) + window_size - 1),\n",
    "         smoothed_rewards, label=f\"Moving Average ({window_size})\", linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Performance on Taxi-v3 with Q-Learning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Greedy Policy (No Exploration)\n",
    "eval_episodes = 100\n",
    "total_eval_rewards = []\n",
    "\n",
    "for episode in range(eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Always exploit (no randomness)\n",
    "        action = np.argmax(Q[state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    total_eval_rewards.append(episode_reward)\n",
    "\n",
    "avg_reward = np.mean(total_eval_rewards)\n",
    "success_rate = np.mean([r > 0 for r in total_eval_rewards])\n",
    "\n",
    "print(f\"Average reward over {eval_episodes} evaluation episodes: {avg_reward:.2f}\")\n",
    "print(f\"Success rate: {success_rate * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2037bef",
   "metadata": {},
   "source": [
    "## Visualizing Taxi States\n",
    "\n",
    "Each **Taxi-v3** state is represented as a single integer (0–499) that encodes four pieces of information:\n",
    "\n",
    "1. **Taxi row** - where the taxi is on the grid (5 possible values)  \n",
    "2. **Taxi column** - horizontal position on the grid (5 possible values)  \n",
    "3. **Passenger location** - one of four landmarks (`R`, `G`, `Y`, `B`) or already in the taxi (5 values)  \n",
    "4. **Destination** - one of the four landmark locations (4 values)\n",
    "\n",
    "That gives a total of 5 * 5 * 5 * 4 = 500 unique states.\n",
    "\n",
    "You can **visualize** the environment using:\n",
    "```python\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4562f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode & Print a Trajectory (Greedy Policy)\n",
    "\n",
    "def run_greedy_episode(render=True, max_steps=20):\n",
    "    \"\"\"Run one greedy (epsilon=0) episode and optionally render each step.\"\"\"\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting Greedy Episode:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "            print(f\"Step {step + 1}: Action = {action}, Reward = {reward}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode finished!\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    return total_reward\n",
    "\n",
    "# Run and visualize one trajectory\n",
    "run_greedy_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6e5d4",
   "metadata": {},
   "source": [
    "## Visualize the Best Policy\n",
    "\n",
    "This section renders a **video** of a greedy rollout using the **best-performing Q-table**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98026140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal greedy rollout video (uses the existing trained Q table `Q`)\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# check if Q-table and seed are set already\n",
    "assert 'Q' in globals(), \"Q-table not found. Train the agent first.\"\n",
    "assert 'SEED' in globals(), \"SEED not found. Ensure the setup cell was run.\"\n",
    "\n",
    "def record_greedy_gif(Q, max_steps=200, seed=SEED, path=\"taxi_greedy.gif\", fps=4):\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        frames.append(env.render())  # capture before action for smoother playback\n",
    "        action = int(np.argmax(Q[state]))\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            frames.append(env.render())\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(path, frames, fps=fps)\n",
    "    return path, total_reward\n",
    "\n",
    "gif_path, ret = record_greedy_gif(Q)\n",
    "print(f\"Greedy rollout return: {ret}\")\n",
    "display(Image(filename=gif_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f8fbe",
   "metadata": {},
   "source": [
    "## Learned Policy Peek\n",
    "\n",
    "Once training is complete, the **Q-table** represents the learned value of every state–action pair.  \n",
    "From it, we can extract a **greedy policy**.\n",
    "\n",
    "This gives the agent’s preferred action for each of the 500 discrete states.\n",
    "\n",
    "Keep in mind:\n",
    "- Not all states are visited equally — many combinations of taxi position, passenger, and destination are rare.  \n",
    "- The Q-values for unvisited states will remain near zero, so their actions may appear arbitrary.  \n",
    "- The most meaningful policy insights come from states that occur frequently during successful episodes.\n",
    "\n",
    ">  You can visualize or analyze a subset of the learned policy — for example, pick a fixed destination and passenger location, and plot which actions the agent prefers across the 5×5 grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba22c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / Load Q-Table and Hyperparameters\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory to save artifacts\n",
    "save_dir = Path(\"artifacts\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Q-table\n",
    "np.save(save_dir / \"taxi_q_table.npy\", Q)\n",
    "\n",
    "# Save training configuration\n",
    "config = {\n",
    "    \"alpha\": alpha,\n",
    "    \"gamma\": gamma,\n",
    "    \"epsilon_min\": epsilon_min,\n",
    "    \"epsilon_decay\": epsilon_decay,\n",
    "    \"episodes\": episodes,\n",
    "    \"max_steps_per_episode\": max_steps_per_episode,\n",
    "}\n",
    "with open(save_dir / \"taxi_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Q-table and config saved to 'artifacts/' directory.\")\n",
    "\n",
    "# Example: to load later\n",
    "# Q_loaded = np.load(\"artifacts/taxi_q_table.npy\")\n",
    "# with open(\"artifacts/taxi_config.json\") as f:\n",
    "#     config_loaded = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42efa44",
   "metadata": {},
   "source": [
    "## Hyperparameter Tips\n",
    "\n",
    "Choosing the right hyperparameters can dramatically affect training stability and speed:\n",
    "\n",
    "- **Learning rate (α):**  \n",
    "  Controls how strongly new information overrides old estimates.  \n",
    "  Typical range: `0.1 – 0.5`. Too high -> unstable; too low -> slow convergence.\n",
    "\n",
    "- **Discount factor (γ):**  \n",
    "  Determines how far into the future the agent looks for rewards.  \n",
    "  Typical range: `0.9 – 0.99`. A high value encourages longer-term planning.\n",
    "\n",
    "- **Exploration rate (ε) and decay:**  \n",
    "  - Start high (1.0) for early exploration.  \n",
    "  - Gradually decay toward a small floor (0.01–0.1).  \n",
    "  - If ε decays too quickly, the agent might stop exploring before discovering optimal paths.\n",
    "\n",
    "- **Episodes:**  \n",
    "  More episodes = better convergence, but longer training.  \n",
    "  Start with `5,000–10,000` and adjust based on reward trends.\n",
    "\n",
    "- **Max steps per episode:**  \n",
    "  Should be large enough for the taxi to complete the task — 100–200 steps usually works well.\n",
    "\n",
    "> If your learning curve plateaus at low reward values, try increasing `ε_decay` (slower decay) or `α` slightly to encourage continued exploration and learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da48864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini Hyperparameter Sweep\n",
    "# Compare a few values of epsilon_decay and alpha, report eval performance, and plot results.\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "def train_and_evaluate(alpha, epsilon_decay, episodes=3000, eval_episodes=100, seed=SEED):\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env.reset(seed=seed)\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    Q_local = np.zeros((n_states, n_actions))\n",
    "\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    max_steps = 100\n",
    "\n",
    "    # Training\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            # epsilon-greedy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q_local[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Q-learning update\n",
    "            best_next = np.argmax(Q_local[next_state])\n",
    "            Q_local[state, action] += alpha * (reward + gamma * Q_local[next_state, best_next] - Q_local[state, action])\n",
    "\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Evaluation (greedy)\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total = 0\n",
    "        for _ in range(max_steps):\n",
    "            action = np.argmax(Q_local[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        eval_rewards.append(total)\n",
    "\n",
    "    env.close()\n",
    "    return float(np.mean(eval_rewards))\n",
    "\n",
    "# Define small sweep space\n",
    "alpha_values = [0.1, 0.2, 0.4]\n",
    "epsilon_decay_values = [0.999, 0.997, 0.995, 0.99]\n",
    "\n",
    "results = {}\n",
    "for a, d in product(alpha_values, epsilon_decay_values):\n",
    "    key = f\"α={a}, ε_decay={d}\"\n",
    "    avg_r = train_and_evaluate(alpha=a, epsilon_decay=d, episodes=3000, eval_episodes=100, seed=SEED)\n",
    "    results[key] = avg_r\n",
    "    print(f\"{key} -> Avg Eval Reward: {avg_r:.2f}\")\n",
    "\n",
    "# Plot results (bar chart)\n",
    "labels = list(results.keys())\n",
    "values = [results[k] for k in labels]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(labels)), values)\n",
    "plt.xticks(range(len(labels)), labels, rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Average Evaluation Reward\")\n",
    "plt.title(\"Mini Sweep: α vs ε_decay on Taxi-v3 (Q-Learning)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
